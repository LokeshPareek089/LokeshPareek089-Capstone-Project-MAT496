{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7d3d7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Environment loaded\n",
      "‚úì GROQ_API_KEY: Set\n",
      "‚úì TAVILY_API_KEY: Set\n",
      "‚úì LANGSMITH_API_KEY: Set\n",
      "‚úì LANGSMITH_TRACING: true\n",
      "‚úì LANGSMITH_PROJECT: pr-overcooked-baggage-31\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Annotated, Optional\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from tavily import TavilyClient\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys and config are loaded\n",
    "print(\"‚úì Environment loaded\")\n",
    "print(f\"‚úì GROQ_API_KEY: {'Set' if os.getenv('GROQ_API_KEY') else 'NOT SET'}\")\n",
    "print(f\"‚úì TAVILY_API_KEY: {'Set' if os.getenv('TAVILY_API_KEY') else 'NOT SET'}\")\n",
    "print(f\"‚úì LANGSMITH_API_KEY: {'Set' if os.getenv('LANGSMITH_API_KEY') else 'NOT SET'}\")\n",
    "print(f\"‚úì LANGSMITH_TRACING: {os.getenv('LANGSMITH_TRACING')}\")\n",
    "print(f\"‚úì LANGSMITH_PROJECT: {os.getenv('LANGSMITH_PROJECT')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04f745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Pydantic models defined\n"
     ]
    }
   ],
   "source": [
    "# Pydantic models for structured outputs\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    \"\"\"Neutral summary of the article\"\"\"\n",
    "    summary: str = Field(description=\"A short, neutral summary of the article (2-3 sentences)\")\n",
    "    word_count: int = Field(description=\"Approximate word count of original article\")\n",
    "\n",
    "class Claim(BaseModel):\n",
    "    \"\"\"Individual claim or statement\"\"\"\n",
    "    text: str = Field(description=\"The claim or statement\")\n",
    "    type: str = Field(description=\"Either 'fact' or 'opinion'\")\n",
    "    confidence: float = Field(description=\"Confidence in classification (0-1)\")\n",
    "\n",
    "class ClaimsExtraction(BaseModel):\n",
    "    \"\"\"Extracted claims from article\"\"\"\n",
    "    factual_claims: List[Claim] = Field(description=\"List of factual claims\")\n",
    "    opinions: List[Claim] = Field(description=\"List of opinions\")\n",
    "    total_claims: int = Field(description=\"Total number of claims extracted\")\n",
    "\n",
    "class FactCheck(BaseModel):\n",
    "    \"\"\"Fact check result for a claim\"\"\"\n",
    "    claim: str = Field(description=\"The original claim\")\n",
    "    status: str = Field(description=\"Either 'supported', 'contradicted', or 'unclear'\")\n",
    "    evidence: str = Field(description=\"Summary of evidence found\")\n",
    "    sources: List[str] = Field(description=\"URLs of sources\")\n",
    "    confidence: float = Field(description=\"Confidence in fact check (0-1)\")\n",
    "\n",
    "class FactCheckResults(BaseModel):\n",
    "    \"\"\"All fact check results\"\"\"\n",
    "    checks: List[FactCheck] = Field(description=\"List of fact check results\")\n",
    "    needs_review: bool = Field(description=\"Whether human review is needed\")\n",
    "\n",
    "class LanguageAnalysis(BaseModel):\n",
    "    \"\"\"Analysis of language bias\"\"\"\n",
    "    loaded_phrases: List[str] = Field(description=\"Emotionally loaded or biased phrases\")\n",
    "    tone: str = Field(description=\"Overall tone: neutral, positive, negative, inflammatory\")\n",
    "    language_bias_score: float = Field(description=\"Language bias score (0-1, 0=neutral)\")\n",
    "    examples: List[str] = Field(description=\"Example sentences showing bias\")\n",
    "\n",
    "class BiasReport(BaseModel):\n",
    "    \"\"\"Final bias analysis report\"\"\"\n",
    "    bias_score: float = Field(description=\"Overall bias score (0-1, 0=unbiased)\")\n",
    "    stance: str = Field(description=\"Predicted stance or position\")\n",
    "    confidence: float = Field(description=\"Confidence in assessment (0-1)\")\n",
    "    key_factors: List[str] = Field(description=\"Key factors contributing to bias score\")\n",
    "    recommendation: str = Field(description=\"Recommendation for readers\")\n",
    "\n",
    "print(\"‚úì Pydantic models defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec011c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì State schema defined\n"
     ]
    }
   ],
   "source": [
    "# LangGraph State\n",
    "class BiasDetectorState(TypedDict):\n",
    "    # Input\n",
    "    article_text: str\n",
    "    article_url: Optional[str]\n",
    "    \n",
    "    # Processing stages\n",
    "    summary: Optional[Summary]\n",
    "    claims: Optional[ClaimsExtraction]\n",
    "    fact_checks: Optional[FactCheckResults]\n",
    "    language_analysis: Optional[LanguageAnalysis]\n",
    "    bias_report: Optional[BiasReport]\n",
    "    \n",
    "    # Control flow\n",
    "    needs_human_review: bool\n",
    "    review_reason: Optional[str]\n",
    "    human_approved: bool\n",
    "    \n",
    "    # Messages for tracing\n",
    "    messages: List[str]\n",
    "\n",
    "print(\"‚úì State schema defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1471ab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LLM initialized (Groq - llama-3.3-70b-versatile)\n",
      "‚úì Tavily client initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM with structured output support\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.1,\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "# Initialize Tavily client for web search\n",
    "tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "\n",
    "print(\"‚úì LLM initialized (Groq - llama-3.3-70b-versatile)\")\n",
    "print(\"‚úì Tavily client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c13c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Article fetcher defined\n"
     ]
    }
   ],
   "source": [
    "def fetch_article_from_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch article text from a URL using web scraping.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text from paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "        article_text = '\\n\\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])\n",
    "        \n",
    "        if len(article_text) < 100:\n",
    "            # Fallback to all text\n",
    "            article_text = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        return article_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error fetching article: {str(e)}\"\n",
    "\n",
    "print(\"‚úì Article fetcher defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "818bc988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì URL input node defined\n"
     ]
    }
   ],
   "source": [
    "def url_input_node(state: BiasDetectorState) -> BiasDetectorState:\n",
    "    \"\"\"\n",
    "    Node that waits for URL input via breakpoint.\n",
    "    \"\"\"\n",
    "    print(\"\\nüì∞ URL Input Node - Ready for URL\")\n",
    "    \n",
    "    # If no URL provided yet, just wait\n",
    "    if not state.get('article_url') or state['article_url'] == \"\":\n",
    "        print(\"‚è∏Ô∏è  PAUSED: Waiting for URL input...\")\n",
    "        return state\n",
    "    \n",
    "    # URL provided, fetch the article\n",
    "    print(f\"\\nüì• Fetching article from: {state['article_url']}\")\n",
    "    article_text = fetch_article_from_url(state['article_url'])\n",
    "    \n",
    "    if article_text.startswith(\"Error\"):\n",
    "        print(f\"‚ùå {article_text}\")\n",
    "        state['article_text'] = \"\"\n",
    "        state['messages'].append(f\"Failed to fetch URL\")\n",
    "    else:\n",
    "        state['article_text'] = article_text\n",
    "        state['messages'].append(f\"Fetched {len(article_text)} characters\")\n",
    "        print(f\"‚úì Fetched {len(article_text)} characters\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úì URL input node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc5fadb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Summarization node defined\n"
     ]
    }
   ],
   "source": [
    "def summarize_node(state: BiasDetectorState) -> BiasDetectorState:\n",
    "    \"\"\"\n",
    "    Generate a neutral summary of the article.\n",
    "    \"\"\"\n",
    "    print(\"\\nüîÑ Running: Summarization Node\")\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(Summary)\n",
    "    \n",
    "    prompt = f\"\"\"You are a neutral news analyst. Read the following article and provide a short, neutral summary.\n",
    "Focus on the main facts and events without adding interpretation or opinion.\n",
    "\n",
    "Article:\n",
    "{state['article_text'][:3000]}\n",
    "\n",
    "Provide a 2-3 sentence neutral summary and estimate the word count.\"\"\"\n",
    "\n",
    "    summary = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    state['summary'] = summary\n",
    "    state['messages'].append(f\"Summary created: {len(summary.summary)} chars\")\n",
    "    \n",
    "    print(f\"‚úì Summary: {summary.summary[:100]}...\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úì Summarization node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe53aaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Claims extraction node defined\n"
     ]
    }
   ],
   "source": [
    "def extract_claims_node(state: BiasDetectorState) -> BiasDetectorState:\n",
    "    \"\"\"\n",
    "    Extract factual claims and opinions from the article.\n",
    "    \"\"\"\n",
    "    print(\"\\nüîÑ Running: Claims Extraction Node\")\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(ClaimsExtraction)\n",
    "    \n",
    "    prompt = f\"\"\"You are a critical analyst. Read this article and extract key claims.\n",
    "\n",
    "Separate them into:\n",
    "1. FACTUAL CLAIMS: Statements that can be verified (dates, events, statistics, quotes)\n",
    "2. OPINIONS: Judgments, interpretations, predictions, or subjective statements\n",
    "\n",
    "Article:\n",
    "{state['article_text'][:4000]}\n",
    "\n",
    "For each claim, provide:\n",
    "- The exact text of the claim\n",
    "- Type: 'fact' or 'opinion'\n",
    "- Confidence: How confident you are in the classification (0-1)\n",
    "\n",
    "Extract 5-10 of the most important claims.\"\"\"\n",
    "\n",
    "    claims = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    state['claims'] = claims\n",
    "    state['messages'].append(f\"Extracted {claims.total_claims} claims: {len(claims.factual_claims)} facts, {len(claims.opinions)} opinions\")\n",
    "    \n",
    "    print(f\"‚úì Extracted {len(claims.factual_claims)} factual claims\")\n",
    "    print(f\"‚úì Extracted {len(claims.opinions)} opinions\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úì Claims extraction node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9b30ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fact checking node defined\n"
     ]
    }
   ],
   "source": [
    "def fact_check_node(state: BiasDetectorState) -> BiasDetectorState:\n",
    "    \"\"\"\n",
    "    Fact-check claims using Tavily web search.\n",
    "    \"\"\"\n",
    "    print(\"\\nüîÑ Running: Fact Checking Node\")\n",
    "    \n",
    "    claims = state['claims']\n",
    "    fact_checks = []\n",
    "    needs_review = False\n",
    "    \n",
    "    # Select top 3-5 most important factual claims\n",
    "    factual_claims = claims.factual_claims[:5]\n",
    "    \n",
    "    for claim_obj in factual_claims:\n",
    "        claim_text = claim_obj.text\n",
    "        print(f\"  üîç Checking: {claim_text[:60]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Search for information about the claim\n",
    "            search_results = tavily_client.search(\n",
    "                query=claim_text,\n",
    "                max_results=3\n",
    "            )\n",
    "            \n",
    "            # Analyze results with LLM\n",
    "            context = \"\\n\\n\".join([\n",
    "                f\"Source: {r.get('url', 'Unknown')}\\n{r.get('content', '')}\" \n",
    "                for r in search_results.get('results', [])\n",
    "            ])\n",
    "            \n",
    "            analysis_prompt = f\"\"\"Based on the following web search results, determine if this claim is supported, contradicted, or unclear.\n",
    "\n",
    "Claim: {claim_text}\n",
    "\n",
    "Search Results:\n",
    "{context[:2000]}\n",
    "\n",
    "Determine:\n",
    "1. Status: 'supported', 'contradicted', or 'unclear'\n",
    "2. Brief evidence summary\n",
    "3. Confidence (0-1)\n",
    "\n",
    "Be conservative: if evidence is mixed or insufficient, mark as 'unclear'.\"\"\"\n",
    "\n",
    "            response = llm.invoke([HumanMessage(content=analysis_prompt)])\n",
    "            \n",
    "            # Parse response\n",
    "            status = \"unclear\"\n",
    "            if \"supported\" in response.content.lower():\n",
    "                status = \"supported\"\n",
    "            elif \"contradicted\" in response.content.lower():\n",
    "                status = \"contradicted\"\n",
    "            \n",
    "            # Extract confidence\n",
    "            confidence = 0.5\n",
    "            if \"high confidence\" in response.content.lower() or \"clearly\" in response.content.lower():\n",
    "                confidence = 0.8\n",
    "            elif \"unclear\" in status or \"insufficient\" in response.content.lower():\n",
    "                confidence = 0.3\n",
    "                needs_review = True\n",
    "            \n",
    "            fact_check = FactCheck(\n",
    "                claim=claim_text,\n",
    "                status=status,\n",
    "                evidence=response.content[:300],\n",
    "                sources=[r.get('url', '') for r in search_results.get('results', [])[:3]],\n",
    "                confidence=confidence\n",
    "            )\n",
    "            \n",
    "            fact_checks.append(fact_check)\n",
    "            print(f\"    ‚úì Status: {status} (confidence: {confidence:.2f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚úó Error: {str(e)}\")\n",
    "            fact_checks.append(FactCheck(\n",
    "                claim=claim_text,\n",
    "                status=\"unclear\",\n",
    "                evidence=f\"Error during fact check: {str(e)}\",\n",
    "                sources=[],\n",
    "                confidence=0.0\n",
    "            ))\n",
    "            needs_review = True\n",
    "    \n",
    "    state['fact_checks'] = FactCheckResults(\n",
    "        checks=fact_checks,\n",
    "        needs_review=needs_review\n",
    "    )\n",
    "    state['needs_human_review'] = needs_review\n",
    "    if needs_review:\n",
    "        state['review_reason'] = \"Low confidence in fact checking results\"\n",
    "    \n",
    "    state['messages'].append(f\"Fact-checked {len(fact_checks)} claims\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úì Fact checking node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c1ad4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Language analysis node defined\n"
     ]
    }
   ],
   "source": [
    "def language_analysis_node(state: BiasDetectorState) -> BiasDetectorState:\n",
    "    \"\"\"\n",
    "    Analyze language for emotional or loaded wording.\n",
    "    \"\"\"\n",
    "    print(\"\\nüîÑ Running: Language Analysis Node\")\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(LanguageAnalysis)\n",
    "    \n",
    "    prompt = f\"\"\"You are a linguistic analyst. Analyze this article for biased or emotionally loaded language.\n",
    "\n",
    "Look for:\n",
    "- Emotionally charged words (e.g., \"catastrophic\", \"hero\", \"villain\")\n",
    "- Loaded adjectives and adverbs\n",
    "- One-sided framing\n",
    "- Inflammatory rhetoric\n",
    "- Persuasive language\n",
    "\n",
    "Article excerpt:\n",
    "{state['article_text'][:4000]}\n",
    "\n",
    "Provide:\n",
    "1. List of loaded phrases (with context)\n",
    "2. Overall tone: neutral, positive, negative, or inflammatory\n",
    "3. Language bias score (0 = perfectly neutral, 1 = extremely biased)\n",
    "4. Example sentences showing bias\"\"\"\n",
    "\n",
    "    analysis = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    state['language_analysis'] = analysis\n",
    "    state['messages'].append(f\"Language analysis: tone={analysis.tone}, bias={analysis.language_bias_score:.2f}\")\n",
    "    \n",
    "    print(f\"‚úì Tone: {analysis.tone}\")\n",
    "    print(f\"‚úì Language bias score: {analysis.language_bias_score:.2f}\")\n",
    "    print(f\"‚úì Found {len(analysis.loaded_phrases)} loaded phrases\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úì Language analysis node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "947db557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Bias scoring node defined\n"
     ]
    }
   ],
   "source": [
    "def bias_scoring_node(state: BiasDetectorState) -> BiasDetectorState:\n",
    "    \"\"\"\n",
    "    Calculate final bias score and predict stance.\n",
    "    \"\"\"\n",
    "    print(\"\\nüîÑ Running: Bias Scoring Node\")\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(BiasReport)\n",
    "    \n",
    "    # Compile all analysis\n",
    "    summary_text = state['summary'].summary if state['summary'] else \"No summary\"\n",
    "    \n",
    "    fact_summary = \"\\n\".join([\n",
    "        f\"- {fc.claim}: {fc.status} (conf: {fc.confidence:.2f})\"\n",
    "        for fc in state['fact_checks'].checks\n",
    "    ]) if state['fact_checks'] else \"No fact checks\"\n",
    "    \n",
    "    lang_summary = f\"Tone: {state['language_analysis'].tone}, Score: {state['language_analysis'].language_bias_score}\" if state['language_analysis'] else \"No analysis\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a media bias expert. Based on all the analysis, provide a final bias assessment.\n",
    "\n",
    "SUMMARY:\n",
    "{summary_text}\n",
    "\n",
    "FACT CHECK RESULTS:\n",
    "{fact_summary}\n",
    "\n",
    "LANGUAGE ANALYSIS:\n",
    "{lang_summary}\n",
    "Loaded phrases: {', '.join(state['language_analysis'].loaded_phrases[:5]) if state['language_analysis'] else 'None'}\n",
    "\n",
    "Calculate:\n",
    "1. Overall bias score (0-1):\n",
    "   - 0.0-0.2: Minimal bias\n",
    "   - 0.2-0.4: Low bias\n",
    "   - 0.4-0.6: Moderate bias\n",
    "   - 0.6-0.8: High bias\n",
    "   - 0.8-1.0: Extreme bias\n",
    "\n",
    "2. Predicted stance (e.g., \"pro-government\", \"anti-corporate\", \"left-leaning\", \"right-leaning\", \"neutral\")\n",
    "\n",
    "3. Confidence in your assessment (0-1)\n",
    "\n",
    "4. Key factors contributing to the score\n",
    "\n",
    "5. Recommendation for readers\n",
    "\n",
    "Consider:\n",
    "- Contradicted facts increase bias\n",
    "- Emotional language increases bias\n",
    "- One-sided coverage increases bias\n",
    "- Missing context increases bias\"\"\"\n",
    "\n",
    "    report = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    state['bias_report'] = report\n",
    "    state['messages'].append(f\"Final bias score: {report.bias_score:.2f}, stance: {report.stance}\")\n",
    "    \n",
    "    # Check if we need review due to high bias or low confidence\n",
    "    if report.bias_score > 0.7 or report.confidence < 0.5:\n",
    "        state['needs_human_review'] = True\n",
    "        state['review_reason'] = f\"High bias score ({report.bias_score:.2f}) or low confidence ({report.confidence:.2f})\"\n",
    "    \n",
    "    print(f\"‚úì Bias score: {report.bias_score:.2f}\")\n",
    "    print(f\"‚úì Stance: {report.stance}\")\n",
    "    print(f\"‚úì Confidence: {report.confidence:.2f}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úì Bias scoring node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ca95ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Human review node defined\n"
     ]
    }
   ],
   "source": [
    "def human_review_node(state: BiasDetectorState) -> BiasDetectorState:\n",
    "    \"\"\"\n",
    "    Pause for human review if needed.\n",
    "    \"\"\"\n",
    "    print(\"\\n‚ö†Ô∏è  HUMAN REVIEW REQUIRED\")\n",
    "    print(f\"Reason: {state.get('review_reason', 'Unknown')}\")\n",
    "    print(\"\\nCurrent Analysis:\")\n",
    "    print(f\"- Bias Score: {state['bias_report'].bias_score:.2f}\")\n",
    "    print(f\"- Stance: {state['bias_report'].stance}\")\n",
    "    print(f\"- Confidence: {state['bias_report'].confidence:.2f}\")\n",
    "    \n",
    "    if state['fact_checks']:\n",
    "        print(f\"\\nFact Checks:\")\n",
    "        for fc in state['fact_checks'].checks:\n",
    "            print(f\"  - {fc.claim[:60]}... ‚Üí {fc.status}\")\n",
    "    \n",
    "    print(\"\\n‚úì Review node reached\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úì Human review node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "784d0bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Routing functions defined\n"
     ]
    }
   ],
   "source": [
    "def should_review(state: BiasDetectorState) -> str:\n",
    "    \"\"\"\n",
    "    Decide whether to route to human review or final output.\n",
    "    \"\"\"\n",
    "    if state.get('needs_human_review', False) and not state.get('human_approved', False):\n",
    "        return \"review\"\n",
    "    return \"end\"\n",
    "\n",
    "print(\"‚úì Routing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f585c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LangGraph workflow compiled with URL input breakpoint\n",
      "‚úì Breakpoints: url_input (for URL), human_review (if needed)\n"
     ]
    }
   ],
   "source": [
    "# Build the LangGraph workflow with URL input breakpoint\n",
    "workflow = StateGraph(BiasDetectorState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"url_input\", url_input_node)  # NEW: First node for URL input\n",
    "workflow.add_node(\"summarize\", summarize_node)\n",
    "workflow.add_node(\"extract_claims\", extract_claims_node)\n",
    "workflow.add_node(\"fact_check\", fact_check_node)\n",
    "workflow.add_node(\"language_analysis\", language_analysis_node)\n",
    "workflow.add_node(\"bias_scoring\", bias_scoring_node)\n",
    "workflow.add_node(\"human_review\", human_review_node)\n",
    "\n",
    "# Define edges - Start with URL input\n",
    "workflow.set_entry_point(\"url_input\")  # CHANGED: Start with URL input node\n",
    "workflow.add_edge(\"url_input\", \"summarize\")  # NEW: url_input -> summarize\n",
    "workflow.add_edge(\"summarize\", \"extract_claims\")\n",
    "workflow.add_edge(\"extract_claims\", \"fact_check\")\n",
    "workflow.add_edge(\"fact_check\", \"language_analysis\")\n",
    "workflow.add_edge(\"language_analysis\", \"bias_scoring\")\n",
    "\n",
    "# Conditional edge for human review\n",
    "workflow.add_conditional_edges(\n",
    "    \"bias_scoring\",\n",
    "    should_review,\n",
    "    {\n",
    "        \"review\": \"human_review\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"human_review\", END)\n",
    "\n",
    "# Compile with checkpointer - TWO breakpoints now\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(\n",
    "    checkpointer=memory, \n",
    "    interrupt_before=[\"url_input\", \"human_review\"]  # CHANGED: Two breakpoints\n",
    ")\n",
    "\n",
    "print(\"‚úì LangGraph workflow compiled with URL input breakpoint\")\n",
    "print(\"‚úì Breakpoints: url_input (for URL), human_review (if needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "706ce9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Display function defined\n"
     ]
    }
   ],
   "source": [
    "def display_results(state: BiasDetectorState):\n",
    "    \"\"\"\n",
    "    Pretty print the analysis results.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üì∞ NEWS BIAS DETECTION REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if state.get('article_url'):\n",
    "        print(f\"\\nüîó Source URL: {state['article_url']}\")\n",
    "    \n",
    "    # Summary\n",
    "    if state.get('summary'):\n",
    "        print(f\"\\nüìù SUMMARY:\")\n",
    "        print(f\"{state['summary'].summary}\")\n",
    "        print(f\"Word count: ~{state['summary'].word_count}\")\n",
    "    \n",
    "    # Claims\n",
    "    if state.get('claims'):\n",
    "        print(f\"\\nüìä CLAIMS EXTRACTED:\")\n",
    "        print(f\"  Factual claims: {len(state['claims'].factual_claims)}\")\n",
    "        print(f\"  Opinions: {len(state['claims'].opinions)}\")\n",
    "        \n",
    "        print(\"\\n  Top Factual Claims:\")\n",
    "        for i, claim in enumerate(state['claims'].factual_claims[:3], 1):\n",
    "            print(f\"    {i}. {claim.text}\")\n",
    "        \n",
    "        print(\"\\n  Top Opinions:\")\n",
    "        for i, opinion in enumerate(state['claims'].opinions[:3], 1):\n",
    "            print(f\"    {i}. {opinion.text}\")\n",
    "    \n",
    "    # Fact Checks\n",
    "    if state.get('fact_checks'):\n",
    "        print(f\"\\nüîç FACT CHECK RESULTS:\")\n",
    "        for fc in state['fact_checks'].checks:\n",
    "            print(f\"\\n  Claim: {fc.claim}\")\n",
    "            print(f\"  Status: {fc.status.upper()}\")\n",
    "            print(f\"  Confidence: {fc.confidence:.2f}\")\n",
    "            if fc.sources:\n",
    "                print(f\"  Sources: {fc.sources[0]}\")\n",
    "    \n",
    "    # Language Analysis\n",
    "    if state.get('language_analysis'):\n",
    "        lang = state['language_analysis']\n",
    "        print(f\"\\nüí¨ LANGUAGE ANALYSIS:\")\n",
    "        print(f\"  Tone: {lang.tone}\")\n",
    "        print(f\"  Language Bias Score: {lang.language_bias_score:.2f}\")\n",
    "        print(f\"  Loaded Phrases: {len(lang.loaded_phrases)}\")\n",
    "        if lang.loaded_phrases:\n",
    "            print(f\"    Examples: {', '.join(lang.loaded_phrases[:3])}\")\n",
    "    \n",
    "    # Final Report\n",
    "    if state.get('bias_report'):\n",
    "        report = state['bias_report']\n",
    "        print(f\"\\n‚öñÔ∏è  FINAL BIAS ASSESSMENT:\")\n",
    "        print(f\"  Bias Score: {report.bias_score:.2f} / 1.0\")\n",
    "        print(f\"  Stance: {report.stance}\")\n",
    "        print(f\"  Confidence: {report.confidence:.2f}\")\n",
    "        \n",
    "        print(f\"\\n  Key Factors:\")\n",
    "        for factor in report.key_factors:\n",
    "            print(f\"    ‚Ä¢ {factor}\")\n",
    "        \n",
    "        print(f\"\\n  üí° Recommendation:\")\n",
    "        print(f\"  {report.recommendation}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"‚úì Display function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6454dbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì URL breakpoint analysis function defined\n"
     ]
    }
   ],
   "source": [
    "def analyze_with_url_breakpoint(thread_id: str = \"url_analysis\"):\n",
    "    \"\"\"\n",
    "    Start analysis with URL input breakpoint.\n",
    "    This is the main function to use for interactive URL input.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting News Bias Detector with URL Input\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Initialize empty state\n",
    "    initial_state = {\n",
    "        \"article_text\": \"\",\n",
    "        \"article_url\": \"\",  # Empty - will be filled at breakpoint\n",
    "        \"summary\": None,\n",
    "        \"claims\": None,\n",
    "        \"fact_checks\": None,\n",
    "        \"language_analysis\": None,\n",
    "        \"bias_report\": None,\n",
    "        \"needs_human_review\": False,\n",
    "        \"review_reason\": None,\n",
    "        \"human_approved\": False,\n",
    "        \"messages\": []\n",
    "    }\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    # Step 2: Run until url_input breakpoint\n",
    "    print(\"\\n‚è∏Ô∏è  Workflow will pause for URL input...\")\n",
    "    for state in app.stream(initial_state, config):\n",
    "        pass\n",
    "    \n",
    "    # Step 3: Get URL from user\n",
    "    url = input(\"\\nüì∞ Enter news article URL: \").strip()\n",
    "    \n",
    "    if not url:\n",
    "        print(\"‚ùå No URL provided\")\n",
    "        return None\n",
    "    \n",
    "    # Step 4: Update state with URL\n",
    "    current_state = app.get_state(config)\n",
    "    current_values = current_state.values\n",
    "    current_values['article_url'] = url\n",
    "    app.update_state(config, current_values)\n",
    "    \n",
    "    print(f\"\\n‚úì URL set: {url}\")\n",
    "    print(\"‚ñ∂Ô∏è  Continuing analysis...\\n\")\n",
    "    \n",
    "    # Step 5: Continue workflow\n",
    "    final_state = None\n",
    "    for state in app.stream(None, config):\n",
    "        final_state = list(state.values())[0]\n",
    "        \n",
    "        # Check for human review breakpoint\n",
    "        if final_state.get('needs_human_review') and not final_state.get('human_approved'):\n",
    "            print(\"\\n‚è∏Ô∏è  Paused at human review\")\n",
    "            approve = input(\"\\n‚ùì Approve and continue? (yes/no): \").strip().lower()\n",
    "            \n",
    "            if approve == 'yes':\n",
    "                current_state = app.get_state(config)\n",
    "                current_values = current_state.values\n",
    "                current_values['human_approved'] = True\n",
    "                app.update_state(config, current_values)\n",
    "                \n",
    "                for state in app.stream(None, config):\n",
    "                    final_state = list(state.values())[0]\n",
    "            else:\n",
    "                print(\"‚ùå Analysis stopped\")\n",
    "                return final_state\n",
    "    \n",
    "    # Step 6: Display and export results\n",
    "    if final_state:\n",
    "        display_results(final_state)\n",
    "        \n",
    "        # Auto-generate filename from thread_id\n",
    "        filename = f\"analysis_{thread_id}.json\"\n",
    "        export_results(final_state, filename)\n",
    "    \n",
    "    return final_state\n",
    "\n",
    "print(\"‚úì URL breakpoint analysis function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a898f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Export function defined\n"
     ]
    }
   ],
   "source": [
    "def export_results(state: BiasDetectorState, filename: str = \"bias_report.json\"):\n",
    "    \"\"\"\n",
    "    Export analysis results to JSON file.\n",
    "    \"\"\"\n",
    "    output = {\n",
    "        \"article_url\": state.get('article_url'),\n",
    "        \"summary\": {\n",
    "            \"text\": state['summary'].summary if state.get('summary') else None,\n",
    "            \"word_count\": state['summary'].word_count if state.get('summary') else None\n",
    "        },\n",
    "        \"claims\": {\n",
    "            \"factual\": [{\"text\": c.text, \"confidence\": c.confidence} \n",
    "                       for c in state['claims'].factual_claims] if state.get('claims') else [],\n",
    "            \"opinions\": [{\"text\": c.text, \"confidence\": c.confidence} \n",
    "                        for c in state['claims'].opinions] if state.get('claims') else []\n",
    "        },\n",
    "        \"fact_checks\": [\n",
    "            {\n",
    "                \"claim\": fc.claim,\n",
    "                \"status\": fc.status,\n",
    "                \"confidence\": fc.confidence,\n",
    "                \"sources\": fc.sources\n",
    "            } for fc in state['fact_checks'].checks\n",
    "        ] if state.get('fact_checks') else [],\n",
    "        \"language_analysis\": {\n",
    "            \"tone\": state['language_analysis'].tone if state.get('language_analysis') else None,\n",
    "            \"bias_score\": state['language_analysis'].language_bias_score if state.get('language_analysis') else None,\n",
    "            \"loaded_phrases\": state['language_analysis'].loaded_phrases if state.get('language_analysis') else []\n",
    "        },\n",
    "        \"bias_report\": {\n",
    "            \"bias_score\": state['bias_report'].bias_score if state.get('bias_report') else None,\n",
    "            \"stance\": state['bias_report'].stance if state.get('bias_report') else None,\n",
    "            \"confidence\": state['bias_report'].confidence if state.get('bias_report') else None,\n",
    "            \"key_factors\": state['bias_report'].key_factors if state.get('bias_report') else [],\n",
    "            \"recommendation\": state['bias_report'].recommendation if state.get('bias_report') else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úì Results exported to {filename}\")\n",
    "\n",
    "print(\"‚úì Export function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a88ed4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting News Bias Detector with URL Input\n",
      "================================================================================\n",
      "\n",
      "‚è∏Ô∏è  Workflow will pause for URL input...\n",
      "\n",
      "‚úì URL set: https://thewire.in/history/past-continuous-1993-bombay-blasts-were-in-response-an-entire-community-being-alienated\n",
      "‚ñ∂Ô∏è  Continuing analysis...\n",
      "\n",
      "\n",
      "üì∞ URL Input Node - Ready for URL\n",
      "\n",
      "üì• Fetching article from: https://thewire.in/history/past-continuous-1993-bombay-blasts-were-in-response-an-entire-community-being-alienated\n",
      "‚úì Fetched 208 characters\n",
      "\n",
      "üîÑ Running: Summarization Node\n",
      "‚úì Summary: The 1993 Bombay blasts were part of a larger pattern of terrorist attacks. The blasts occurred in 19...\n",
      "\n",
      "üîÑ Running: Claims Extraction Node\n",
      "‚úì Extracted 2 factual claims\n",
      "‚úì Extracted 1 opinions\n",
      "\n",
      "üîÑ Running: Fact Checking Node\n",
      "  üîç Checking: 1993 Bombay Blasts...\n",
      "    ‚úì Status: supported (confidence: 0.80)\n",
      "  üîç Checking: The Wire News India, Latest News,News from India, Politics, ...\n",
      "    ‚úì Status: supported (confidence: 0.80)\n",
      "\n",
      "üîÑ Running: Language Analysis Node\n",
      "‚úì Tone: negative\n",
      "‚úì Language bias score: 0.60\n",
      "‚úì Found 2 loaded phrases\n",
      "\n",
      "üîÑ Running: Bias Scoring Node\n",
      "‚úì Bias score: 0.50\n",
      "‚úì Stance: left-leaning\n",
      "‚úì Confidence: 0.80\n",
      "\n",
      "================================================================================\n",
      "üì∞ NEWS BIAS DETECTION REPORT\n",
      "================================================================================\n",
      "\n",
      "üîó Source URL: https://thewire.in/history/past-continuous-1993-bombay-blasts-were-in-response-an-entire-community-being-alienated\n",
      "\n",
      "üìù SUMMARY:\n",
      "The 1993 Bombay blasts were part of a larger pattern of terrorist attacks. The blasts occurred in 1993 and were a significant event in India's history. The article discusses the context and implications of the blasts\n",
      "Word count: ~500\n",
      "\n",
      "üìä CLAIMS EXTRACTED:\n",
      "  Factual claims: 2\n",
      "  Opinions: 1\n",
      "\n",
      "  Top Factual Claims:\n",
      "    1. 1993 Bombay Blasts\n",
      "    2. The Wire News India, Latest News,News from India, Politics, External Affairs, Science, Economics, Gender and Culture\n",
      "\n",
      "  Top Opinions:\n",
      "    1. Part of a Continuum of Terror\n",
      "\n",
      "üîç FACT CHECK RESULTS:\n",
      "\n",
      "  Claim: 1993 Bombay Blasts\n",
      "  Status: SUPPORTED\n",
      "  Confidence: 0.80\n",
      "  Sources: https://www.youtube.com/watch?v=KhSFC1GNgNM\n",
      "\n",
      "  Claim: The Wire News India, Latest News,News from India, Politics, External Affairs, Science, Economics, Gender and Culture\n",
      "  Status: SUPPORTED\n",
      "  Confidence: 0.80\n",
      "  Sources: https://m.thewire.in/amp\n",
      "\n",
      "üí¨ LANGUAGE ANALYSIS:\n",
      "  Tone: negative\n",
      "  Language Bias Score: 0.60\n",
      "  Loaded Phrases: 2\n",
      "    Examples: Continuum of Terror, Part of a Continuum\n",
      "\n",
      "‚öñÔ∏è  FINAL BIAS ASSESSMENT:\n",
      "  Bias Score: 0.50 / 1.0\n",
      "  Stance: left-leaning\n",
      "  Confidence: 0.80\n",
      "\n",
      "  Key Factors:\n",
      "    ‚Ä¢ negative tone\n",
      "    ‚Ä¢ loaded phrases\n",
      "    ‚Ä¢ one-sided coverage\n",
      "\n",
      "  üí° Recommendation:\n",
      "  Readers should consider multiple sources for a balanced view\n",
      "\n",
      "================================================================================\n",
      "‚úì Results exported to analysis_my_analysis.json\n"
     ]
    }
   ],
   "source": [
    "# Main usage: Analyze with URL input breakpoint\n",
    "# Run this cell to start - it will ask for URL input\n",
    "\n",
    "result = analyze_with_url_breakpoint(thread_id=\"my_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ce14ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä COMPLETE BIAS ANALYSIS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚öñÔ∏è  Bias Score: 0.5 / 1.0\n",
      "üìç Stance: left-leaning\n",
      "üéØ Confidence: 0.8\n",
      "\n",
      "üîë Key Factors:\n",
      "   ‚Ä¢ negative tone\n",
      "   ‚Ä¢ loaded phrases\n",
      "   ‚Ä¢ one-sided coverage\n",
      "\n",
      "üí° Recommendation:\n",
      "   Readers should consider multiple sources for a balanced view\n",
      "\n",
      "üí¨ Language Analysis:\n",
      "   Tone: negative\n",
      "   Language Bias: 0.6\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display complete bias report from JSON\n",
    "import json\n",
    "\n",
    "try:\n",
    "    with open('analysis_my_analysis.json', 'r', encoding='utf-8') as f:\n",
    "        saved_results = json.load(f)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä COMPLETE BIAS ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    bias = saved_results.get('bias_report', {})\n",
    "    print(f\"\\n‚öñÔ∏è  Bias Score: {bias.get('bias_score', 'N/A')} / 1.0\")\n",
    "    print(f\"üìç Stance: {bias.get('stance', 'N/A')}\")\n",
    "    print(f\"üéØ Confidence: {bias.get('confidence', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nüîë Key Factors:\")\n",
    "    for factor in bias.get('key_factors', []):\n",
    "        print(f\"   ‚Ä¢ {factor}\")\n",
    "    \n",
    "    print(f\"\\nüí° Recommendation:\")\n",
    "    print(f\"   {bias.get('recommendation', 'N/A')}\")\n",
    "    \n",
    "    lang = saved_results.get('language_analysis', {})\n",
    "    print(f\"\\nüí¨ Language Analysis:\")\n",
    "    print(f\"   Tone: {lang.get('tone', 'N/A')}\")\n",
    "    print(f\"   Language Bias: {lang.get('bias_score', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå JSON file not found - run analysis first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
