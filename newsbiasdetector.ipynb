{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa1ea0f",
   "metadata": {},
   "source": [
    "# NewsBiasDetector — main.ipynb\n",
    "\n",
    "Clean final notebook for MAT496 capstone. This notebook includes:\n",
    "- Nodes: summarizer, claim extractor, web-search, fact-check, language-bias, bias-scoring\n",
    "- Human-in-the-loop (interactive & file-driven)\n",
    "- Export to JSON/CSV and LangSmith logging helper\n",
    "- BeautifulSoup-only URL extractor and URL-input breakpoint\n",
    "- Final pipeline run that prints the explicit output parameters required for submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2f2a826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: d:\\MAT 496\\NewsBiasDetector\\venv\\Scripts\\python.exe\n",
      "CWD: d:\\MAT 496\\NewsBiasDetector\n",
      "OPENAI_API_KEY present: True\n",
      "SERPAPI_API_KEY present: False\n",
      "LANGSMITH_API_KEY present: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — load environment (do not print secrets)\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "load_dotenv()\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"OPENAI_API_KEY present:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n",
    "print(\"SERPAPI_API_KEY present:\", bool(os.getenv(\"SERPAPI_API_KEY\")))\n",
    "print(\"LANGSMITH_API_KEY present:\", bool(os.getenv(\"LANGSMITH_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6747902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith tracer not available or LangChain version mismatch: No module named 'langchain.callbacks'\n",
      "Install and use `from langchain.callbacks import LangSmithTracer` or check LangChain version.\n"
     ]
    }
   ],
   "source": [
    "# Optional: enable LangSmith tracer for full tracing of LLM calls (if you want full traces)\n",
    "try:\n",
    "    # try to import LangSmith tracer from LangChain callbacks (API may vary with versions)\n",
    "    from langchain.callbacks import LangSmithTracer  # typical import in many LangChain versions\n",
    "    tracer = LangSmithTracer(project_name = os.getenv(\"LANGSMITH_PROJECT_NAME\") or os.getenv(\"LANGSMITH_PROJECT\") or \"NewsBiasDetector\")\n",
    "    # Attach to your llm or to callbacks for chains/agents:\n",
    "    # If llm constructor accepts callbacks, you can pass callbacks=[tracer] when creating llm.\n",
    "    # Example (if using ChatOpenAI):\n",
    "    # llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0, callbacks=[tracer])\n",
    "    print(\"LangSmith tracer created; to enable tracing attach `callbacks=[tracer]` to your llm or chain.\")\n",
    "except Exception as e:\n",
    "    print(\"LangSmith tracer not available or LangChain version mismatch:\", e)\n",
    "    print(\"Install and use `from langchain.callbacks import LangSmithTracer` or check LangChain version.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44d09307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracer attempt result: No tracer class found; env-vars for tracing are set (may need restart).\n"
     ]
    }
   ],
   "source": [
    "# Tracer autodetect + attach (paste & run after your LLM init cell)\n",
    "import os, importlib\n",
    "def try_make_tracer_and_attach(llm_obj):\n",
    "    candidates = [\n",
    "        (\"langchain.callbacks\", \"LangSmithTracer\"),\n",
    "        (\"langchain.callbacks\", \"LangChainTracer\"),\n",
    "        (\"langchain_core.callbacks\", \"LangChainTracer\"),\n",
    "        (\"langgraph.tracing\", \"LangSmithTracer\"),\n",
    "        (\"langsmith\", \"LangSmithTracer\"),\n",
    "    ]\n",
    "    tracer = None\n",
    "    msg = \"\"\n",
    "    for mod_name, cls_name in candidates:\n",
    "        try:\n",
    "            mod = importlib.import_module(mod_name)\n",
    "            TracerClass = getattr(mod, cls_name, None) or getattr(mod, \"LangSmithTracer\", None) or getattr(mod, \"LangChainTracer\", None)\n",
    "            if TracerClass:\n",
    "                try:\n",
    "                    project = os.getenv(\"LANGSMITH_PROJECT_NAME\") or os.getenv(\"LANGSMITH_PROJECT\") or \"newsbiasdetector\"\n",
    "                    tracer = TracerClass(project_name=project)\n",
    "                    msg = f\"Tracer created via {mod_name}.{TracerClass.__name__} (project={project})\"\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    msg = f\"Found {mod_name}.{cls_name} but failed to instantiate: {e}\"\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if tracer is None:\n",
    "        if os.getenv(\"LANGCHAIN_TRACING\") or os.getenv(\"LANGSMITH_TRACING\") or os.getenv(\"LANGCHAIN_TRACING_V2\"):\n",
    "            return llm_obj, None, \"No tracer class found; env-vars for tracing are set (may need restart).\"\n",
    "        return llm_obj, None, \"No tracer class found; install compatible langchain/langsmith versions.\"\n",
    "\n",
    "    # Try to attach tracer to ChatOpenAI (recreate llm with callbacks)\n",
    "    try:\n",
    "        ChatCls = None\n",
    "        for test_mod in (\"langchain.chat_models\", \"langchain.chat_models.openai\", \"langchain.chat_models.base\"):\n",
    "            try:\n",
    "                m = importlib.import_module(test_mod)\n",
    "                ChatCls = getattr(m, \"ChatOpenAI\", None) or getattr(m, \"ChatModel\", None)\n",
    "                if ChatCls:\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "        if ChatCls:\n",
    "            try:\n",
    "                new_llm = ChatCls(model_name=os.getenv(\"OPENAI_MODEL_NAME\",\"gpt-4o\"), temperature=0, callbacks=[tracer])\n",
    "                return new_llm, tracer, msg + \" — attached to LLM.\"\n",
    "            except Exception as e:\n",
    "                return llm_obj, tracer, msg + f\" — tracer created but failed to attach to LLM: {e}\"\n",
    "        else:\n",
    "            return llm_obj, tracer, msg + \" — tracer created but ChatOpenAI class not found to attach.\"\n",
    "    except Exception as e:\n",
    "        return llm_obj, tracer, f\"Tracer created but reattach failed: {e}\"\n",
    "\n",
    "# Run attempt\n",
    "current_llm = globals().get(\"llm\", None)\n",
    "llm, tracer, message = try_make_tracer_and_attach(current_llm)\n",
    "if tracer:\n",
    "    globals()[\"llm\"] = llm\n",
    "print(\"Tracer attempt result:\", message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "153c7228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM not initialized — continuing. Error: Chat model not available\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — imports and safe LLM initialization (may be None if libs not installed)\n",
    "import json, re, requests\n",
    "from typing import TypedDict, List, Dict, Any, Optional\n",
    "import os\n",
    "try:\n",
    "    from langchain_core.messages import SystemMessage, HumanMessage\n",
    "except Exception:\n",
    "    try:\n",
    "        from langchain.schema import SystemMessage, HumanMessage\n",
    "    except Exception:\n",
    "        SystemMessage = None\n",
    "        HumanMessage = None\n",
    "\n",
    "try:\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "except Exception:\n",
    "    ChatOpenAI = None\n",
    "    OpenAIEmbeddings = None\n",
    "\n",
    "try:\n",
    "    from langgraph.graph import StateGraph, END\n",
    "except Exception:\n",
    "    StateGraph = None\n",
    "    END = None\n",
    "\n",
    "MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\", \"gpt-4o\")\n",
    "EMBED_MODEL = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "llm = None\n",
    "embeddings = None\n",
    "try:\n",
    "    if ChatOpenAI is None:\n",
    "        raise RuntimeError(\"Chat model not available\")\n",
    "    llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0)\n",
    "    embeddings = OpenAIEmbeddings(model=EMBED_MODEL)\n",
    "    print(\"LLM initialized:\", MODEL_NAME)\n",
    "except Exception as e:\n",
    "    llm = None\n",
    "    embeddings = None\n",
    "    print(\"LLM not initialized — continuing. Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c975442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — helper to extract text from LLM responses\n",
    "def _resp_to_text(resp) -> str:\n",
    "    if resp is None:\n",
    "        return \"\"\n",
    "    content = getattr(resp, \"content\", None)\n",
    "    if isinstance(content, str):\n",
    "        return content.strip()\n",
    "    try:\n",
    "        if hasattr(resp, \"generations\"):\n",
    "            gens = resp.generations\n",
    "            if isinstance(gens, list) and len(gens):\n",
    "                g0 = gens[0]\n",
    "                if isinstance(g0, dict) and \"text\" in g0:\n",
    "                    return str(g0[\"text\"]).strip()\n",
    "                if hasattr(g0, \"text\"):\n",
    "                    return str(g0.text).strip()\n",
    "        if hasattr(resp, \"choices\"):\n",
    "            c0 = resp.choices[0]\n",
    "            t = getattr(c0, \"text\", None)\n",
    "            if isinstance(t, str):\n",
    "                return t.strip()\n",
    "            m = getattr(c0, \"message\", None)\n",
    "            if isinstance(m, str):\n",
    "                return m.strip()\n",
    "            if isinstance(m, dict):\n",
    "                return json.dumps(m)\n",
    "        if isinstance(resp, str):\n",
    "            return resp.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return repr(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eaa0f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — summarizer node\n",
    "SUMMARY_PROMPT = \"\"\"\n",
    "You are a neutral assistant. Read the article text delimited by <<<ARTICLE>>> and produce:\n",
    "- A short, factual SUMMARY (2-3 sentences).\n",
    "- An estimated confidence score (0.0 - 1.0).\n",
    "Return JSON only: {\"summary\": \"...\", \"confidence\": 0.87}\n",
    "<<<ARTICLE>>>\n",
    "{article}\n",
    "\"\"\"\n",
    "def summarize_article(article_text: str) -> Dict[str, Any]:\n",
    "    if llm is None:\n",
    "        return {\"summary\": (article_text or \"\")[:400], \"confidence\": 0.0}\n",
    "    prompt = SUMMARY_PROMPT.format(article=article_text.strip()[:6000])\n",
    "    messages = [SystemMessage(content=\"You are a factual summariser. Return only JSON.\"), HumanMessage(content=prompt)]\n",
    "    try:\n",
    "        resp = llm.invoke(messages)\n",
    "        raw = _resp_to_text(resp)\n",
    "        parsed = json.loads(raw)\n",
    "        return {\"summary\": parsed.get(\"summary\", \"\"), \"confidence\": float(parsed.get(\"confidence\", 0.0))}\n",
    "    except Exception:\n",
    "        return {\"summary\": article_text[:400], \"confidence\": 0.0}\n",
    "\n",
    "def summarize_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    art = state.get(\"raw_text\",\"\") or \"\"\n",
    "    res = summarize_article(art)\n",
    "    state[\"summary\"] = res.get(\"summary\")\n",
    "    state[\"summary_confidence\"] = float(res.get(\"confidence\", 0.0))\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f1fb58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — claim extractor node\n",
    "CLAIM_PROMPT = \"\"\"\n",
    "Extract factual claims and opinion statements from the article. Return JSON exactly:\n",
    "{\"claims\": [{\"id\":1,\"speaker\":\"Unknown\",\"text\":\"...\",\"type\":\"factual\",\"topic\":\"...\"}, ...]}\n",
    "Article between <<<ARTICLE>>> and <<<END>>>.\n",
    "<<<ARTICLE>>>\n",
    "{article}\n",
    "<<<END>>>\n",
    "\"\"\"\n",
    "def extract_claims(article_text: str) -> List[Dict[str, Any]]:\n",
    "    if llm is None:\n",
    "        return []\n",
    "    prompt = CLAIM_PROMPT.format(article=article_text.strip()[:6000])\n",
    "    messages = [SystemMessage(content=\"Return valid JSON only.\"), HumanMessage(content=prompt)]\n",
    "    try:\n",
    "        resp = llm.invoke(messages)\n",
    "        raw = _resp_to_text(resp)\n",
    "        j = json.loads(raw)\n",
    "        return j.get(\"claims\", [])\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def extract_claims_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    art = state.get(\"raw_text\",\"\") or \"\"\n",
    "    state[\"claims\"] = extract_claims(art)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99fb459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — web_search helper (SerpAPI/Tavily fallback)\n",
    "import requests\n",
    "TAVILY_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "SERPAPI_KEY = os.getenv(\"SERPAPI_API_KEY\")\n",
    "def web_search(query: str, num_results: int = 3) -> List[Dict[str, Any]]:\n",
    "    q = (query or \"\").strip()\n",
    "    if not q:\n",
    "        return []\n",
    "    if TAVILY_KEY:\n",
    "        try:\n",
    "            from tavily import TavilyClient\n",
    "            client = TavilyClient(api_key=TAVILY_KEY)\n",
    "            resp = client.search(query=q, max_results=num_results)\n",
    "            return [{\"title\": r.get(\"title\",\"\"), \"snippet\": r.get(\"snippet\",\"\"), \"url\": r.get(\"url\",\"\")} for r in resp.get(\"results\", [])[:num_results]]\n",
    "        except Exception:\n",
    "            pass\n",
    "    if SERPAPI_KEY:\n",
    "        try:\n",
    "            params = {\"q\": q, \"api_key\": SERPAPI_KEY, \"engine\": \"google\", \"num\": num_results}\n",
    "            r = requests.get(\"https://serpapi.com/search.json\", params=params, timeout=15.0)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            items = data.get(\"organic_results\") or data.get(\"organic\") or []\n",
    "            return [{\"title\": it.get(\"title\",\"\"), \"snippet\": it.get(\"snippet\",\"\"), \"url\": it.get(\"link\") or it.get(\"url\") or \"\"} for it in items[:num_results]]\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise RuntimeError(\"No web search provider configured. Set TAVILY_API_KEY or SERPAPI_API_KEY.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2755fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — fact check helper and node (uses web_search + llm)\n",
    "FACTCHECK_PROMPT = \"\"\"\n",
    "You are a verifier. Given a claim and search evidence, decide SUPPORTED/CONTRADICTED/UNCERTAIN and return JSON:\n",
    "{\"verdict\": \"SUPPORTED\"|\"CONTRADICTED\"|\"UNCERTAIN\", \"confidence\": 0.0, \"extracted_evidence\": [{\"url\":\"..\",\"snippet\":\"..\",\"note\":\"..\"}]}\n",
    "\"\"\"\n",
    "def fact_check_claim(claim_text: str, top_k: int = 3) -> Dict[str, Any]:\n",
    "    try:\n",
    "        results = web_search(claim_text, num_results=top_k)\n",
    "    except Exception:\n",
    "        results = []\n",
    "    evidence_blocks = [f\"URL: {r.get('url','')}\\nTITLE: {r.get('title','')}\\nSNIPPET: {r.get('snippet','')}\\n---\" for r in results]\n",
    "    evidence_text = \"\\n\".join(evidence_blocks) if evidence_blocks else \"No results found.\"\n",
    "    if llm is None:\n",
    "        fallback = {\"verdict\": \"UNCERTAIN\", \"confidence\": 0.0, \"extracted_evidence\": [{\"url\": r.get('url',''), \"snippet\": r.get('snippet','')} for r in results], \"raw_search\": results}\n",
    "        return fallback\n",
    "    prompt = FACTCHECK_PROMPT\n",
    "    full_input = \"Claim:\\n\" + claim_text + \"\\n\\nSearch evidence:\\n\" + evidence_text + \"\\n\\nReturn JSON only.\"\n",
    "    messages = [SystemMessage(content=\"You are a precise verifier. Return JSON only.\"), HumanMessage(content=prompt + \"\\n\\n\" + full_input)]\n",
    "    try:\n",
    "        resp = llm.invoke(messages)\n",
    "        raw = _resp_to_text(resp)\n",
    "        parsed = json.loads(raw)\n",
    "        parsed[\"raw_search\"] = results\n",
    "        return parsed\n",
    "    except Exception:\n",
    "        return {\"verdict\": \"UNCERTAIN\", \"confidence\": 0.0, \"extracted_evidence\": [{\"url\": r.get('url',''), \"snippet\": r.get('snippet','')} for r in results], \"raw_search\": results}\n",
    "\n",
    "def fact_check_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    claims = state.get(\"claims\", []) or []\n",
    "    res = []\n",
    "    for c in claims:\n",
    "        text = c.get(\"text\") if isinstance(c, dict) else str(c)\n",
    "        cid = c.get(\"id\") if isinstance(c, dict) else None\n",
    "        fc = fact_check_claim(text, top_k=3)\n",
    "        res.append({\"id\": cid, \"claim\": text, \"verdict\": fc.get(\"verdict\"), \"confidence\": float(fc.get(\"confidence\", 0.0) or 0.0), \"evidence\": fc.get(\"extracted_evidence\", []), \"raw_search\": fc.get(\"raw_search\", [])})\n",
    "    state[\"fact_results\"] = res\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2582d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — language bias analysis node (LLM + lexical fallback)\n",
    "LANGUAGE_BIAS_PROMPT = \"\"\"\n",
    "Inspect article language for bias. Return JSON:\n",
    "{\"tone\":\"neutral|positive|negative|mixed\",\"sentiment_score\":float,\"emotion_words\":[],\"hedging_phrases\":[],\"subjectivity\":float,\"examples\":[]}\n",
    "Analyze article between <<<ARTICLE>>> and <<<END>>>.\n",
    "<<<ARTICLE>>>\n",
    "{article}\n",
    "<<<END>>>\n",
    "\"\"\"\n",
    "_LOADED_WORDS = {\"positive\":[\"welcomed\",\"benefit\",\"celebrate\"], \"negative\":[\"shameful\",\"outrage\",\"scandal\"], \"hedges\":[\"may\",\"might\",\"could\",\"appears\",\"suggests\",\"likely\",\"reportedly\"]}\n",
    "def analyze_language(article_text: str, claims: List[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    if llm is None:\n",
    "        text = article_text.lower()\n",
    "        found = [w for w in _LOADED_WORDS['positive']+_LOADED_WORDS['negative'] if w in text]\n",
    "        hedges = [h for h in _LOADED_WORDS['hedges'] if re.search(r\"\\b\"+re.escape(h)+r\"\\b\", text)]\n",
    "        pos = sum(text.count(w) for w in _LOADED_WORDS['positive'])\n",
    "        neg = sum(text.count(w) for w in _LOADED_WORDS['negative'])\n",
    "        sentiment = 0.0\n",
    "        if pos+neg>0:\n",
    "            sentiment = (pos-neg)/(pos+neg)\n",
    "        subjectivity = min(1.0, (len(found)+len(hedges))/10.0)\n",
    "        tone = 'neutral'\n",
    "        if sentiment>0.2: tone='positive'\n",
    "        elif sentiment<-0.2: tone='negative'\n",
    "        examples=[]\n",
    "        for s in re.split(r'(?<=[.!?])\\s+', article_text):\n",
    "            sl=s.lower()\n",
    "            if any(w in sl for w in found) or any(h in sl for h in hedges):\n",
    "                examples.append(s.strip()[:120])\n",
    "                if len(examples)>=3: break\n",
    "        return {\"tone\":tone,\"sentiment_score\":float(sentiment),\"emotion_words\":found[:8],\"hedging_phrases\":hedges[:8],\"subjectivity\":float(subjectivity),\"examples\":examples}\n",
    "    prompt = LANGUAGE_BIAS_PROMPT.format(article=article_text.strip()[:7000])\n",
    "    messages = [SystemMessage(content=\"Return JSON only.\"), HumanMessage(content=prompt)]\n",
    "    try:\n",
    "        resp = llm.invoke(messages)\n",
    "        raw = _resp_to_text(resp)\n",
    "        parsed = json.loads(raw)\n",
    "        parsed['sentiment_score'] = float(parsed.get('sentiment_score',0.0))\n",
    "        parsed['subjectivity'] = float(parsed.get('subjectivity',0.0))\n",
    "        return parsed\n",
    "    except Exception:\n",
    "        # fallback\n",
    "        return analyze_language(article_text, claims=None)\n",
    "\n",
    "def language_bias_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    art = state.get('raw_text','') or ''\n",
    "    analysis = analyze_language(art)\n",
    "    state['language_bias'] = analysis\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "357f787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 — bias scoring helpers and node\n",
    "def _clamp(x,a=-1.0,b=1.0): return max(a,min(b,x))\n",
    "def compute_fact_component(fact_results):\n",
    "    if not fact_results:\n",
    "        return {\"fact_component\":0.0,\"mean_fact_conf\":0.0,\"n_claims\":0}\n",
    "    weighted_sum=0.0; weight_total=0.0; confs=[]\n",
    "    for r in fact_results:\n",
    "        verdict=(r.get('verdict') or '').upper(); conf=float(r.get('confidence',0.0) or 0.0); confs.append(conf)\n",
    "        score = 1.0 if verdict=='SUPPORTED' else (-1.0 if verdict=='CONTRADICTED' else 0.0)\n",
    "        weighted_sum += score * conf; weight_total += conf\n",
    "    mean_conf = (sum(confs)/len(confs)) if confs else 0.0\n",
    "    if weight_total>0: fact_component = weighted_sum / weight_total\n",
    "    else:\n",
    "        n_sup = sum(1 for r in fact_results if (r.get('verdict') or '').upper()=='SUPPORTED')\n",
    "        n_con = sum(1 for r in fact_results if (r.get('verdict') or '').upper()=='CONTRADICTED')\n",
    "        total = max(1, len(fact_results))\n",
    "        fact_component = (n_sup - n_con)/ total\n",
    "    return {\"fact_component\": _clamp(fact_component), \"mean_fact_conf\": mean_conf, \"n_claims\": len(fact_results)}\n",
    "def compute_language_component(lang):\n",
    "    if not lang: return {\"lang_component\":0.0,\"subjectivity\":0.0,\"sentiment\":0.0}\n",
    "    sent = float(lang.get('sentiment_score',0.0) or 0.0); subj = float(lang.get('subjectivity',0.0) or 0.0)\n",
    "    return {\"lang_component\": _clamp(sent*subj), \"subjectivity\": subj, \"sentiment\": sent}\n",
    "def compute_bias_score(state, w_fact=0.7, w_lang=0.3):\n",
    "    fr = state.get('fact_results',[]) or []\n",
    "    lang = state.get('language_bias',{}) or {}\n",
    "    fi = compute_fact_component(fr); li = compute_language_component(lang)\n",
    "    combined = _clamp(fi['fact_component']*w_fact + li['lang_component']*w_lang)\n",
    "    mean_fact_conf = fi.get('mean_fact_conf',0.0); subj = li.get('subjectivity',0.0); n_claims = fi.get('n_claims',0)\n",
    "    if n_claims>0: conf = 0.7*mean_fact_conf + 0.3*(1.0 - subj)\n",
    "    else: conf = 0.3*mean_fact_conf + 0.7*(1.0 - subj)\n",
    "    conf = max(0.0, min(1.0, conf))\n",
    "    if combined > 0.2: stance='pro-article'\n",
    "    elif combined < -0.2: stance='against-article'\n",
    "    else: stance='neutral'\n",
    "    breakdown = {\"fact_component\":fi['fact_component'], \"lang_component\":li['lang_component'], \"weights\":{\"w_fact\":w_fact,\"w_lang\":w_lang}, \"mean_fact_confidence\": fi.get('mean_fact_conf',0.0), \"language_subjectivity\": li.get('subjectivity',0.0), \"n_claims\": fi.get('n_claims',0)}\n",
    "    return {\"bias_score\": combined, \"stance\": stance, \"confidence\": conf, \"breakdown\": breakdown}\n",
    "def bias_scoring_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    r = compute_bias_score(state)\n",
    "    state['bias_score'] = float(r['bias_score']); state['leaning'] = r['stance']; state['bias_breakdown'] = r['breakdown']; state['bias_confidence'] = float(r['confidence'])\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8eedb5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph created. Nodes: ['summarize', 'extract_claims', 'fact_check', 'language_bias', 'bias_scoring']\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 — graph wiring (optional) + wrappers\n",
    "class Claim(TypedDict):\n",
    "    id: int\n",
    "    speaker: str\n",
    "    text: str\n",
    "    type: str\n",
    "    topic: str\n",
    "class NewsState(TypedDict, total=False):\n",
    "    raw_text: str\n",
    "    summary: str\n",
    "    summary_confidence: float\n",
    "    claims: List[Claim]\n",
    "    fact_results: List[Dict[str, Any]]\n",
    "    language_bias: Dict[str, Any]\n",
    "    bias_score: float\n",
    "    leaning: str\n",
    "    bias_breakdown: Dict[str, Any]\n",
    "    bias_confidence: float\n",
    "graph = None\n",
    "if StateGraph is not None:\n",
    "    try:\n",
    "        graph = StateGraph(NewsState)\n",
    "        graph.add_node('summarize', summarize_node)\n",
    "        graph.add_node('extract_claims', extract_claims_node)\n",
    "        graph.add_node('fact_check', fact_check_node)\n",
    "        graph.add_node('language_bias', language_bias_node)\n",
    "        graph.add_node('bias_scoring', bias_scoring_node)\n",
    "        graph.add_edge('summarize','extract_claims'); graph.add_edge('extract_claims','fact_check'); graph.add_edge('fact_check','language_bias'); graph.add_edge('language_bias','bias_scoring'); graph.add_edge('bias_scoring', END)\n",
    "        graph.set_entry_point('summarize')\n",
    "        print('Graph created. Nodes:', list(graph.nodes.keys()))\n",
    "    except Exception as e:\n",
    "        print('Graph wiring skipped/warning:', e)\n",
    "else:\n",
    "    print('LangGraph not available — skip graph wiring.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e319051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 — human-review interactive and file nodes + maybe_human_review wrapper + export/logging helpers\n",
    "import json, csv, time\n",
    "from pathlib import Path\n",
    "REVIEW_QUEUE = Path('human_review_queue.json')\n",
    "REVIEW_DECISIONS = Path('human_review_decisions.json')\n",
    "def human_review_node_blocking(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    fr = state.get('fact_results',[]) or []\n",
    "    for r in fr:\n",
    "        verdict = (r.get('verdict') or 'UNCERTAIN').upper(); conf = float(r.get('confidence',0.0) or 0.0)\n",
    "        if verdict=='UNCERTAIN' or conf < 0.5:\n",
    "            print('\\n--- HUMAN REVIEW REQUIRED ---')\n",
    "            print('Claim id:', r.get('id'))\n",
    "            print('Claim text:', r.get('claim'))\n",
    "            print('Current verdict:', verdict, 'confidence:', conf)\n",
    "            print('Evidence (top 2):')\n",
    "            for ev in (r.get('evidence') or [])[:2]:\n",
    "                print('-', (ev.get('snippet') or '')[:200], ev.get('url',''))\n",
    "            ans = input('Decision [y=ACCEPT / n=REJECT / s=SKIP]: ').strip().lower()\n",
    "            if ans in ('y','yes'):\n",
    "                r.setdefault('human_review',{})['decision']='ACCEPT'\n",
    "            elif ans in ('n','no'):\n",
    "                r.setdefault('human_review',{})['decision']='REJECT'; r['verdict']='CONTRADICTED'; r.setdefault('human_review',{})['note']='Human rejected'\n",
    "            else:\n",
    "                r.setdefault('human_review',{})['decision']='SKIPPED'\n",
    "        else:\n",
    "            r.setdefault('human_review',{})['decision']='NOT_NEEDED'\n",
    "    state['fact_results']=fr; return state\n",
    "def human_review_node_file(state: Dict[str, Any], auto_wait: bool=False, timeout: int=60) -> Dict[str, Any]:\n",
    "    fr = state.get('fact_results',[]) or []\n",
    "    to_review=[]\n",
    "    for r in fr:\n",
    "        verdict=(r.get('verdict') or 'UNCERTAIN').upper(); conf=float(r.get('confidence',0.0) or 0.0)\n",
    "        if verdict=='UNCERTAIN' or conf<0.5:\n",
    "            to_review.append({'id':r.get('id'),'claim':r.get('claim'),'verdict':verdict,'confidence':conf,'evidence':r.get('evidence',[])[:3]})\n",
    "    if not to_review: return state\n",
    "    REVIEW_QUEUE.write_text(json.dumps({'items':to_review}, indent=2), encoding='utf-8')\n",
    "    print(f'Wrote {len(to_review)} items to {REVIEW_QUEUE}. Fill {REVIEW_DECISIONS} with decisions.')\n",
    "    if auto_wait:\n",
    "        waited=0; interval=3\n",
    "        while waited<timeout:\n",
    "            if REVIEW_DECISIONS.exists():\n",
    "                try:\n",
    "                    dec=json.loads(REVIEW_DECISIONS.read_text(encoding='utf-8'))\n",
    "                    decisions={d['id']:d for d in dec.get('decisions',[])}\n",
    "                    for r in fr:\n",
    "                        rid=r.get('id')\n",
    "                        if rid in decisions:\n",
    "                            d=decisions[rid]; newv=d.get('decision','').upper()\n",
    "                            if newv in ('SUPPORTED','CONTRADICTED','UNCERTAIN'):\n",
    "                                r['verdict']=newv; r.setdefault('human_review',{})['decision']='APPLIED'; r.setdefault('human_review',{})['note']=d.get('note','')\n",
    "                    state['fact_results']=fr; print('Applied file decisions.'); return state\n",
    "                except Exception as e:\n",
    "                    print('Failed to parse decisions file:', e)\n",
    "            time.sleep(interval); waited+=interval\n",
    "        print('Timed out waiting for decisions file; continuing.')\n",
    "    return state\n",
    "def maybe_human_review(state: Dict[str, Any], method: str = 'interactive', threshold_confidence: float = 0.5) -> Dict[str, Any]:\n",
    "    blocking_ok = 'human_review_node_blocking' in globals()\n",
    "    file_ok = 'human_review_node_file' in globals()\n",
    "    if not blocking_ok and not file_ok:\n",
    "        print('No human-review nodes defined — skipping review.'); return state\n",
    "    fr = state.get('fact_results',[]) or []\n",
    "    try:\n",
    "        low_conf = any(float(r.get('confidence',0.0) or 0.0) < float(threshold_confidence) for r in fr)\n",
    "    except Exception:\n",
    "        low_conf = False\n",
    "    try:\n",
    "        verdicts = {(r.get('verdict') or 'UNCERTAIN').upper() for r in fr}; mixed = ('SUPPORTED' in verdicts and 'CONTRADICTED' in verdicts)\n",
    "    except Exception:\n",
    "        mixed = False\n",
    "    try:\n",
    "        subj = float(state.get('language_bias',{}).get('subjectivity',0.0) or 0.0)\n",
    "    except Exception:\n",
    "        subj = 0.0\n",
    "    trigger = low_conf or mixed or (subj>0.5)\n",
    "    print(f'maybe_human_review: low_conf={low_conf}, mixed={mixed}, subj={subj:.2f} -> trigger={trigger}')\n",
    "    if not trigger: return state\n",
    "    method = (method or 'interactive').lower()\n",
    "    if method=='file' and file_ok:\n",
    "        return human_review_node_file(state)\n",
    "    if method in ('interactive','blocking','input') and blocking_ok:\n",
    "        return human_review_node_blocking(state)\n",
    "    if blocking_ok: return human_review_node_blocking(state)\n",
    "    if file_ok: return human_review_node_file(state)\n",
    "    return state\n",
    "EXPORT_JSON = Path('last_run.json')\n",
    "EXPORT_CSV = Path('last_run.csv')\n",
    "def _claim_to_row(claim, fact_result, language_bias, state_meta):\n",
    "    return {\n",
    "        'claim_id': claim.get('id'), 'claim_text': claim.get('text'), 'claim_type': claim.get('type'),\n",
    "        'verdict': (fact_result or {}).get('verdict'), 'fact_confidence': (fact_result or {}).get('confidence'),\n",
    "        'human_review_decision': (fact_result or {}).get('human_review',{}).get('decision') if isinstance(fact_result,dict) else None,\n",
    "        'evidence_snippets': ' || '.join([ (e.get('snippet') or '')[:200] for e in (fact_result or {}).get('evidence', []) ]),\n",
    "        'language_tone': language_bias.get('tone'), 'language_subjectivity': language_bias.get('subjectivity'),\n",
    "        'bias_score': state_meta.get('bias_score'), 'leaning': state_meta.get('leaning'), 'bias_confidence': state_meta.get('bias_confidence'),\n",
    "        'summary': (state_meta.get('summary') or '')[:400]\n",
    "    }\n",
    "def export_run(state: Dict[str, Any], filename_json: str = None, filename_csv: str = None) -> Dict[str, str]:\n",
    "    if filename_json is None: filename_json = str(EXPORT_JSON)\n",
    "    if filename_csv is None: filename_csv = str(EXPORT_CSV)\n",
    "    with open(filename_json, 'w', encoding='utf-8') as f: json.dump(state, f, ensure_ascii=False, indent=2)\n",
    "    claims = state.get('claims',[]) or []; fact_results = state.get('fact_results',[]) or []; language_bias = state.get('language_bias',{}) or {}\n",
    "    by_id = {fr.get('id'):fr for fr in fact_results if isinstance(fr,dict)}\n",
    "    rows = []\n",
    "    if claims:\n",
    "        for c in claims:\n",
    "            fr = by_id.get(c.get('id'), {})\n",
    "            rows.append(_claim_to_row(c, fr, language_bias, state))\n",
    "    else:\n",
    "        rows.append({'claim_id':None,'claim_text':None,'claim_type':None,'verdict':None,'fact_confidence':None,'human_review_decision':None,'evidence_snippets':None,'language_tone':language_bias.get('tone'),'language_subjectivity':language_bias.get('subjectivity'),'bias_score':state.get('bias_score'),'leaning':state.get('leaning'),'bias_confidence':state.get('bias_confidence'),'summary':(state.get('summary') or '')[:400]})\n",
    "    with open(filename_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = list(rows[0].keys()); writer = csv.DictWriter(csvfile, fieldnames=fieldnames); writer.writeheader();\n",
    "        for r in rows: writer.writerow(r)\n",
    "    print(f'Exported JSON -> {filename_json}'); print(f'Exported CSV -> {filename_csv}'); return {'json': filename_json, 'csv': filename_csv}\n",
    "def log_run_to_langsmith(state: Dict[str, Any], project: str = None):\n",
    "    try:\n",
    "        import langsmith\n",
    "    except Exception:\n",
    "        print('langsmith not installed — skipping LangSmith logging.'); return None\n",
    "    if not os.getenv('LANGSMITH_API_KEY'):\n",
    "        print('LANGSMITH_API_KEY not set — skipping LangSmith logging.'); return None\n",
    "    try:\n",
    "        client = getattr(langsmith, 'Client', langsmith)()\n",
    "    except Exception:\n",
    "        client = langsmith\n",
    "    try:\n",
    "        meta = {'summary': (state.get('summary') or '')[:500], 'bias_score': state.get('bias_score'), 'leaning': state.get('leaning')}\n",
    "        if hasattr(client, 'create_run'):\n",
    "            client.create_run(project_name=project or os.getenv('LANGSMITH_PROJECT','NewsBiasDetector'), metadata=meta)\n",
    "            print('Logged run to LangSmith (create_run called).')\n",
    "        else:\n",
    "            print('LangSmith client found but create_run missing; skipped upload.')\n",
    "    except Exception as e:\n",
    "        print('LangSmith logging failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3007a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 — BeautifulSoup-only extractor (fetch_article_text_bs)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from typing import Optional\n",
    "def _clean_whitespace(text: str) -> str:\n",
    "    text = re.sub(r'\\r\\n|\\r', '\\n', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    return text.strip()\n",
    "def fetch_article_text_bs(url: str, timeout: float = 15.0, user_agent: Optional[str] = None) -> str:\n",
    "    headers = {'User-Agent': user_agent or 'NewsBiasDetector/1.0 (+https://example.com)'}\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=timeout)\n",
    "        resp.raise_for_status(); html = resp.text\n",
    "    except Exception as e:\n",
    "        print('fetch_article_text_bs: download failed:', e); return ''\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "    except Exception:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "    article_tag = soup.find('article')\n",
    "    if article_tag:\n",
    "        text = article_tag.get_text(separator='\\n'); text = _clean_whitespace(text)\n",
    "        if len(text) > 80:\n",
    "            print(f'fetch_article_text_bs: used <article> tag, extracted {len(text)} chars.'); return text\n",
    "    candidates = soup.find_all(['div','main','section'], recursive=True)\n",
    "    best_text = ''\n",
    "    for c in candidates:\n",
    "        ps = c.find_all('p')\n",
    "        if not ps: continue\n",
    "        joined = '\\n\\n'.join(p.get_text().strip() for p in ps if p.get_text().strip())\n",
    "        joined = _clean_whitespace(joined)\n",
    "        if len(joined) > len(best_text): best_text = joined\n",
    "    if best_text and len(best_text) > 80:\n",
    "        print(f'fetch_article_text_bs: used best container heuristic, extracted {len(best_text)} chars.'); return best_text\n",
    "    paragraphs = soup.find_all('p')\n",
    "    if paragraphs:\n",
    "        longest=''; current=[]; last_parent=None\n",
    "        for p in paragraphs:\n",
    "            parent = p.parent\n",
    "            if parent == last_parent or last_parent is None:\n",
    "                current.append(p.get_text().strip())\n",
    "            else:\n",
    "                if current:\n",
    "                    candidate = '\\n\\n'.join([x for x in current if x])\n",
    "                    if len(candidate) > len(longest): longest = candidate\n",
    "                current = [p.get_text().strip()]\n",
    "            last_parent = parent\n",
    "        if current:\n",
    "            candidate = '\\n\\n'.join([x for x in current if x])\n",
    "            if len(candidate) > len(longest): longest = candidate\n",
    "        longest = _clean_whitespace(longest)\n",
    "        if len(longest) > 80:\n",
    "            print(f'fetch_article_text_bs: used longest paragraph sequence fallback, extracted {len(longest)} chars.'); return longest\n",
    "    full = _clean_whitespace(soup.get_text(separator='\\n'))\n",
    "    if len(full) > 80:\n",
    "        print(f'fetch_article_text_bs: fallback to full page text, extracted {len(full)} chars (may include noise).'); return full\n",
    "    print('fetch_article_text_bs: extraction returned short/empty text — page may be JS-heavy or blocked.'); return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c651136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 — URL input breakpoint (asks user for URL and loads it into state)\n",
    "def ask_for_url_breakpoint(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    print('=== URL INPUT BREAKPOINT ===')\n",
    "    print('Paste a news article URL to process, or press Enter to skip and use existing raw_text.')\n",
    "    url = input('Paste URL (or press Enter to skip): ').strip()\n",
    "    if url:\n",
    "        print('Fetching article from:', url)\n",
    "        text = fetch_article_text_bs(url)\n",
    "        if text:\n",
    "            state['raw_text'] = text\n",
    "            print(f'Loaded article ({len(text)} chars).')\n",
    "        else:\n",
    "            print('⚠️ Could not extract article text — keeping previous raw_text.')\n",
    "    else:\n",
    "        print('Skipping URL input — using existing raw_text.')\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9e835dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14 — runner using BeautifulSoup extractor\n",
    "def run_pipeline_from_url_bs(url: str = None, human_method: str = 'interactive') -> Optional[Dict[str, Any]]:\n",
    "    # initial empty state\n",
    "    state = {\"raw_text\": \"\"}\n",
    "    # If URL provided, fetch immediately; otherwise use breakpoint to ask URL\n",
    "    if url:\n",
    "        print('Fetching:', url)\n",
    "        state['raw_text'] = fetch_article_text_bs(url)\n",
    "        if not state['raw_text']:\n",
    "            print('No text extracted from URL; aborting.'); return None\n",
    "    else:\n",
    "        state = ask_for_url_breakpoint(state)\n",
    "    # run pipeline\n",
    "    try:\n",
    "        state = summarize_node(state)\n",
    "        print('SUMMARY (short):', (state.get('summary') or '')[:300])\n",
    "    except Exception as e:\n",
    "        print('summarize_node error:', e)\n",
    "    try:\n",
    "        state = extract_claims_node(state)\n",
    "        print('Extracted claims:', len(state.get('claims',[]) or []))\n",
    "    except Exception as e:\n",
    "        print('extract_claims_node error:', e)\n",
    "    try:\n",
    "        state = fact_check_node(state)\n",
    "        print('Fact-check done. Claims checked:', len(state.get('fact_results',[]) or []))\n",
    "    except Exception as e:\n",
    "        print('fact_check_node error:', e)\n",
    "    try:\n",
    "        state = maybe_human_review(state, method=human_method)\n",
    "    except Exception as e:\n",
    "        print('maybe_human_review error:', e)\n",
    "    try:\n",
    "        state = language_bias_node(state)\n",
    "    except Exception as e:\n",
    "        print('language_bias_node error:', e)\n",
    "    try:\n",
    "        state = bias_scoring_node(state)\n",
    "    except Exception as e:\n",
    "        print('bias_scoring_node error:', e)\n",
    "    try:\n",
    "        exported = export_run(state)\n",
    "    except Exception as e:\n",
    "        print('export_run error:', e); exported = {}\n",
    "    try:\n",
    "        log_run_to_langsmith(state)\n",
    "    except Exception as e:\n",
    "        print('log_run_to_langsmith error:', e)\n",
    "    print('\\n--- FINAL KEY PARAMETERS (explicit) ---')\n",
    "    print('Bias Score:', state.get('bias_score'))\n",
    "    print('Bias Confidence:', state.get('bias_confidence'))\n",
    "    print('Stance/Leaning:', state.get('leaning'))\n",
    "    print('\\nSummary:')\n",
    "    print(state.get('summary'))\n",
    "    print('\\nBias Breakdown:')\n",
    "    import pprint\n",
    "    pprint.pprint(state.get('bias_breakdown', {}))\n",
    "    print('\\nClaims (list):')\n",
    "    pprint.pprint(state.get('claims', []))\n",
    "    print('\\nFact Results (list):')\n",
    "    pprint.pprint(state.get('fact_results', []))\n",
    "    print('\\nLanguage Bias Analysis:')\n",
    "    pprint.pprint(state.get('language_bias', {}))\n",
    "    print('\\nExport files:' , exported)\n",
    "    return state\n",
    "\n",
    "# Example usage (uncomment and replace URL to run):\n",
    "# final_state = run_pipeline_from_url_bs('https://www.example.com/news/article', human_method='interactive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0b40b3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtracers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlangchain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LangChainTracer\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Set environment variables (can also be loaded from .env)\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.schema'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "\n",
    "# Set environment variables (can also be loaded from .env)\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"newsbiasdetector\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# Initialize tracer\n",
    "tracer = LangChainTracer()\n",
    "\n",
    "# Create a simple LLM chain\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Run a traced call\n",
    "response = llm.invoke([HumanMessage(content=\"What is media bias?\")], config={\"callbacks\": [tracer]})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba01da",
   "metadata": {},
   "source": [
    "## How to run\n",
    "1. Ensure your `venv` kernel is selected in VS Code/Jupyter.\n",
    "2. Run cells top-to-bottom.\n",
    "3. To use URL breakpoint: run **Cell 14** without providing a URL — it will pause and ask for a URL input.\n",
    "4. To run with a direct URL: call `run_pipeline_from_url_bs('<your-url>')` in a new cell.\n",
    "5. After the run, `last_run.json` and `last_run.csv` will be created in project root.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
